---
# title: "About"
permalink: /   # This makes the page accessible as the root page
layouts_gallery:
  - url: /assets/images/mm-layout-splash.png
    image_path: /assets/images/mm-layout-splash.png
    alt: "splash layout example"
  - url: /assets/images/mm-layout-single-meta.png
    image_path: /assets/images/mm-layout-single-meta.png
    alt: "single layout with comments and related posts"
  - url: /assets/images/mm-layout-archive.png
    image_path: /assets/images/mm-layout-archive.png
    alt: "archive layout example"
---

<!-- ######################################################################################################################################## -->
I am a Ph.D. student in computer science at the University of Massachusetts Amherst, advised by <a href="https://people.cs.umass.edu/~amir/" target="_blank">Prof. Amir Houmansadr</a>. 

My research is centered on **Privacy & Security in AI** and **Trustworthy Machine Learning**. Lately, my work has involved studying the trustworthiness of multimodal models, while I am also fascinated by various adjacent topics such as **fairness**, **interpretability**, and **responsible AI**. Currently, I am investigating the trustworthiness of multimodal generative models across various domains, including text-to-image and audio-based modalities.

Prior to my graduate studies, I earned my bachelor's degree in computer engineering from the Hong Kong University of Science and Technology (HKUST) in the year 2023, where I completed my Final Year Thesis (FYT) on the topic of "Adversarial Attacks in Federated Learning" under the supervision of <a href="https://eejzhang.people.ust.hk/" target="_black">Prof. Jun Zhang</a>. I have also worked with [Prof. Minhao Cheng](https://cmhcbb.github.io/) on the robustness of language models, specifically exploring methods associated with backdoor defense in text domain.

--------
[[R√©sum√©]](files/CV - Jaechul Roh (11.19.2024).pdf) / [[Google Scholar]](https://scholar.google.com/citations?user=knCeRjsAAAAJ&hl=ko) / [[GitHub]](https://github.com/jrohsc) / [[Linkedin]](https://www.linkedin.com/in/jaechul-roh-572363155/)

<!-- <span style="font-family: 'Courier New', Courier, monospace;">ideas.txt</span> -->
<!-- ############################################################################################### -->
## üì£ News

* **Sep 27 '24**: Our paper "OSLO: One-Shot Label-Only Membership Inference Attacks" was accepted to NeurIPS '24! üéâ

* **Dec 22 '23**: Our paper "Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication" was accepted to the  AAAI '23 PPAI Workshop! üéâ

<!-- ######################################################################################################################################## -->

## üìù Publications

**OSLO: One-Shot Label-Only Membership Inference Attacks** <br>
   Yuefeng Peng, <u><b>Jaechul Roh</b></u>, Subhransu Maji, Amir Houmansadr <br>
   *NeurIPS 2024* <br>
   <!-- NeurIPS 2024 -->
   <a href="https://arxiv.org/pdf/2405.16978" target="_blank">[paper]</a>


**Backdooring Bias into Text-to-Image Models**  
   Ali Naseh, **<u><b>Jaechul Roh</b></u>**, Eugene Bagdasaryan, Amir Houmansadr  
   *Preprint at arXiv (Under Review)*  
   <!-- NeurIPS 2024 -->
   [[paper]](https://arxiv.org/pdf/2406.15213) [[code]](https://github.com/jcroh0508/Backdororing_Bias/)

**Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication**  
   Ali Naseh, **<u><b>Jaechul Roh</b></u>**, Amir Houmansadr  
   *The 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence*  
   <!-- The 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence -->
   [[paper]](https://arxiv.org/pdf/2312.03692)

**Understanding (Un)Intended Memorization in Text-to-Image Generative Models**  
   Ali Naseh, **<u><b>Jaechul Roh</b></u>**, Amir Houmansadr  
   *Preprint at arXiv*  
   [[paper]](https://arxiv.org/pdf/2312.07550)

**Robust Smart Home Face Recognition under Starving Federated Data**  
   **<u><b>Jaechul Roh</b></u>**, Yajun Fang  
   *Oral Presentation in the IEEE International Conference on Universal Village (IEEE UV2022)*  
   <!-- Oral Presentation in the IEEE International Conference on Universal Village (IEEE UV2022)   -->
   [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175525) | [[code]](https://github.com/jcroh0508/FLATS) | [[slides]](https://www.jrohs.com/_files/ugd/c489e1_bbc7e44075944cad98da82f31e7430ae.pdf) | [[video]](https://www.youtube.com/watch?v=Tj9QiJEUBXU&ab_channel=jroh)

**MSDT: Masked Language Model Scoring Defense in Text Domain**  
   **<u><b>Jaechul Roh</b></u>**, Minhao Cheng, Yajun Fang  
   *Oral Presentation in the IEEE International Conference on Universal Village (IEEE UV2022*)  
   <!-- Oral Presentation in the IEEE International Conference on Universal Village (IEEE UV2022)   -->
   [[paper]](https://ieeexplore.ieee.org/document/10175524) | [[code]](https://github.com/jcroh0508/MSDT) | [[slides]](https://www.jrohs.com/_files/ugd/c489e1_bbc7e44075944cad98da82f31e7430ae.pdf) | [[video]](https://www.youtube.com/watch?v=oO3FbxnMdv0&ab_channel=jroh)

**Impact of Adversarial Training on the Robustness of Deep Neural Networks**  
   **<u><b>Jaechul Roh</b></u>**  
   *2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)*  
   <!-- 2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)   -->
   [[paper]](https://ieeexplore.ieee.org/abstract/document/9927611) | [[code]](https://github.com/jcroh0508/Adversarial_Training_Impact)


<!-- ## ü§ñ Projects
**FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models**  
   **<u><b>Jaechul Roh</b></u>**, Andrew Yuan, Jinsong Mao \
   *CS 670 (Computer Vision) Course Project*   -->
   <!-- 2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)   -->
   <!-- [[paper]](files/cs_670.pdf) [[code]](https://github.com/jrohsc/FameBias/) -->